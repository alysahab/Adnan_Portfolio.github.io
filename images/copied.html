<!DOCTYPE HTML>
<html>
    <head>
        <title>PSL Cricket Data Pipeline: Web Scraping + ETL</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    </head>
    <body class="is-preload">

        <!-- Wrapper -->
            <div id="wrapper">

                <!-- Header -->
                    <header id="header">
                        <a href="https://github.com/alysahab/Web-Scraping-PSL-Data" class="logo">PSL Data Pipeline: Web Scraping + ETL</a>
                    </header>

                <!-- Nav -->
                    <nav id="nav">
                        <ul class="links">
                            <li><a href="index.html">Portfolio</a></li>
                            <li><a href="generic.html">Resume</a></li>
                            <li><a href="elements.html">Contact</a></li>
                        </ul>
                        <ul class="icons">
                            <li><a href="#" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
                            <li><a href="https://github.com/alysahab" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
                        </ul>
                    </nav>

                <!-- Main -->
                    <div id="main">

                        <!-- Post -->
                            <section class="post">
                                <header class="major">
                                    <span class="date">June 2024</span>
                                    <h1>Cricket Data Pipeline</h1>
                                    <p>Automated collection and processing of 9 PSL seasons (2016-2024) with 7,000+ records</p>
                                </header>

                                <!-- Project Overview -->
                                <div class="box">
                                    <h2>Pipeline Architecture</h2>
                                    <ul>
                                        <li>Extract: Selenium + BeautifulSoup → Uncleaned Data</li>
                                        <li>Transform: Pandas → Cleaned Data</li>
                                        <li>Load: SQLAlchemy → Load Data into AWS RDS</li>
                                    </ul>
                                    
                                    <div class="row">
                                        <div class="col-4 col-12-medium">
                                            <h3>1. Extraction Layer</h3>
                                            <ul>
                                                <li>Selenium with Headless Edge</li>
                                                <li>ThreadPoolExecutor (8 workers)</li>
                                                <li>BeautifulSoup Parsing</li>
                                            </ul>
                                        </div>
                                        <div class="col-4 col-12-medium">
                                            <h3>2. Transformation</h3>
                                            <ul>
                                                <li>Pandas DataFrames</li>
                                                <li>Data Validation Rules</li>
                                                <li>CSV Intermediate Storage</li>
                                            </ul>
                                        </div>
                                        <div class="col-4 col-12-medium">
                                            <h3>3. Loading</h3>
                                            <ul>
                                                <li>AWS RDS Aurora</li>
                                                <li>SQLAlchemy ORM</li>
                                                <li>Batch Processing</li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>

                                <!-- Technical Implementation -->
                                <h2>Core Implementation Details</h2>
                                
                                <h3>Data Extraction Engine</h3>
                                <p>Multithreaded web scraper handling dynamic content:</p>
                                <pre><code>
from concurrent.futures import ThreadPoolExecutor

class DataExtract:
    def concurrentExtraction(self):
        with ThreadPoolExecutor(max_workers=8) as executor:
            executor.map(self.extract_season_data, season_urls)
                                </code></pre>

                                <h3>Data Transformation Process</h3>
                                <p>Pandas-based cleaning pipeline:</p>
                                <pre><code>
class Transformation:
    def clean_batting_record(self):
        df = pd.read_csv('raw_batting.csv')
        # Handle missing values
        df['strike_rate'] = df['strike_rate'].fillna(0)
        # Type conversion
        df['runs'] = df['runs'].astype(int)
        df.to_csv('cleaned_batting.csv', index=False)
                                </code></pre>

                                <h3>AWS Data Loading</h3>
                                <p>Your implemented loader class with security enhancements:</p>
                                <pre><code>
class Load:
    def __init__(self):
        load_dotenv(override=True)
        # Security: Environment variables
        USER = os.getenv('USER')
        PASSWORD = os.getenv('PASSWORD')
        # Connection pooling
        self.engine = create_engine(
            f'mysql+pymysql://{USER}:{PASSWORD}@{HOST}/{DBNAME}',
            pool_size=10,
            max_overflow=20
        )

    def load_data(self):
        # Batch loading with chunk optimization
        df.to_sql(..., chunksize=1000, method='multi')
                                </code></pre>

                                <!-- Results & Impact -->
                                <h2>Key Achievements</h2>
                                <div class="table-wrapper">
                                    <table class="alt">
                                        <thead>
                                            <tr>
                                                <th>Metric</th>
                                                <th>Result</th>
                                                <th>Impact</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>Data Collection Speed</td>
                                                <td>5.2 pages/sec</td>
                                                <td>67% faster than sequential</td>
                                            </tr>
                                            <tr>
                                                <td>Data Accuracy</td>
                                                <td>98.4% valid records</td>
                                                <td>Reliable analytics base</td>
                                            </tr>
                                            <tr>
                                                <td>Storage Efficiency</td>
                                                <td>62% size reduction</td>
                                                <td>Lower AWS costs</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>

                                <!-- Project Links -->
                                <ul class="actions special">
                                    <li><a href="https://github.com/alysahab/Web-Scraping-PSL-Data" class="button icon solid fa-code">View Source Code</a></li>
                                    <li><a href="https://www.kaggle.com/datasets/alysahab/complete-psl-data-2016-2024" class="button icon solid fa-chart-bar">Explore Analytics</a></li>
                                </ul>

                                <!-- Future Enhancements -->
                                <div class="box">
                                    <h3>Future Roadmap</h3>
                                    <ul>
                                        <li>Real-time updates with AWS Lambda</li>
                                        <li>Player Performance Prediction Models</li>
                                        <li>Interactive Dashboard using Tableau</li>
                                    </ul>
                                </div>

                            </section>

                    </div>
    </body>
</html>

